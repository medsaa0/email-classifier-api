{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8488a706-55be-46ca-8db9-0a6de3a38155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                            message            category\n",
      "0   1  Bonjour, je veux un remboursement. Mon colis e...         r√©clamation\n",
      "1   2            Le produit ne s‚Äôallume pas, que faire ?  probl√®me technique\n",
      "2   3  Est-ce que cette enceinte Bluetooth est compat...    question produit\n",
      "3   4                  Merci pour votre service rapide !               autre\n",
      "4   5  Je veux changer l‚Äôadresse de livraison de ma c...         r√©clamation\n",
      "category\n",
      "probl√®me technique    449\n",
      "r√©clamation           441\n",
      "autre                 436\n",
      "question produit      426\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier avec le bon s√©parateur (point-virgule) et l'encodage\n",
    "df = pd.read_csv(\"data.csv\", encoding=\"utf\", sep=\";\")\n",
    "\n",
    "# V√©rification de l'affichage\n",
    "print(df.head())\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337ef21-85c7-41d9-a46a-45943ea5bee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62b2aa3-53e2-4d25-a0c1-3f77938c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path\n",
    "# import base64\n",
    "# import pandas as pd\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "# from email import message_from_bytes\n",
    "\n",
    "# # Scope pour lecture Gmail\n",
    "# SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "# def authenticate_gmail():\n",
    "#     creds = None\n",
    "#     if os.path.exists('token.json'):\n",
    "#         creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "#     if not creds or not creds.valid:\n",
    "#         if creds and creds.expired and creds.refresh_token:\n",
    "#             creds.refresh(Request())\n",
    "#         else:\n",
    "#             flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "#             creds = flow.run_local_server(port=0)\n",
    "#         with open('token.json', 'w') as token:\n",
    "#             token.write(creds.to_json())\n",
    "#     return creds\n",
    "\n",
    "# def get_emails(service, max_results=10):\n",
    "#     results = service.users().messages().list(userId='me', maxResults=max_results).execute()\n",
    "#     messages = results.get('messages', [])\n",
    "#     data = []\n",
    "\n",
    "#     for msg in messages:\n",
    "#         txt = service.users().messages().get(userId='me', id=msg['id'], format='full').execute()\n",
    "#         headers = txt['payload']['headers']\n",
    "#         subject = [h['value'] for h in headers if h['name'] == 'Subject']\n",
    "#         sender = [h['value'] for h in headers if h['name'] == 'From']\n",
    "\n",
    "#         parts = txt['payload'].get('parts', [])\n",
    "#         body = ''\n",
    "#         if parts:\n",
    "#             for part in parts:\n",
    "#                 if part['mimeType'] == 'text/plain':\n",
    "#                     data_raw = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n",
    "#                     body = data_raw\n",
    "#         else:\n",
    "#             body = base64.urlsafe_b64decode(txt['payload']['body']['data']).decode('utf-8')\n",
    "\n",
    "#         data.append({\n",
    "#             'from': sender[0] if sender else '',\n",
    "#             'subject': subject[0] if subject else '',\n",
    "#             'body': body.strip()\n",
    "#         })\n",
    "\n",
    "#     return pd.DataFrame(data)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     creds = authenticate_gmail()\n",
    "#     service = build('gmail', 'v1', credentials=creds)\n",
    "#     df = get_emails(service, max_results=10)\n",
    "    \n",
    "#     print(df.head())\n",
    "#     df.to_csv('emails.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18cb44-983b-41b0-a95e-e863c613e890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dae6fb6-697f-40f7-9513-303a7171985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                message  \\\n",
      "0     Bonjour, je veux un remboursement. Mon colis e...   \n",
      "1               Le produit ne s‚Äôallume pas, que faire ?   \n",
      "2     Est-ce que cette enceinte Bluetooth est compat...   \n",
      "3                     Merci pour votre service rapide !   \n",
      "4     Je veux changer l‚Äôadresse de livraison de ma c...   \n",
      "...                                                 ...   \n",
      "1746  Votre livreur a sonn√© √† 7h30 un dimanche matin...   \n",
      "1747  L'√©cran tactile de ma table de cuisson √† induc...   \n",
      "1748  Merci pour votre service de reprise de mon anc...   \n",
      "1749  Ce v√©hicule √©lectrique peut-il se recharger su...   \n",
      "1750  Ma garantie √©tendue de 3 ans expire dans 2 moi...   \n",
      "\n",
      "                                          message_clean  \n",
      "0         bonjour veux remboursement colis arrive casse  \n",
      "1                                 produit sallume faire  \n",
      "2      estce cette enceinte bluetooth compatible iphone  \n",
      "3                                  merci service rapide  \n",
      "4              veux changer ladresse livraison commande  \n",
      "...                                                 ...  \n",
      "1746  livreur a sonne a h dimanche matin livraison s...  \n",
      "1747  lecran tactile table cuisson a induction repon...  \n",
      "1748  merci service reprise ancien materiel prix pro...  \n",
      "1749  vehicule electrique peutil recharger prise dom...  \n",
      "1750  garantie etendue ans expire mois appareil comm...  \n",
      "\n",
      "[1751 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def enlever_accents(texte):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', texte) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def nettoyer_texte(texte):\n",
    "    texte = texte.lower()\n",
    "    texte = enlever_accents(texte)\n",
    "    texte = re.sub(r'\\d+', '', texte)               # Supprimer les chiffres\n",
    "    texte = re.sub(r'[^\\w\\s]', '', texte)           # Supprimer ponctuation\n",
    "    texte = re.sub(r'\\s+', ' ', texte).strip()      # Nettoyer les espaces\n",
    "    mots = texte.split()\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return \" \".join(mots)\n",
    "\n",
    "df['message_clean'] = df['message'].apply(nettoyer_texte)\n",
    "print(df[['message', 'message_clean']].head(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8afbd1e-4447-4036-91df-58b78799cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'message', 'category', 'message_clean'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad89adcf-9950-49e6-b312-23442fa6e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['category_encoded'] = le.fit_transform(df['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe01268-d759-4550-a20f-28d179e3ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message_clean'], df['category_encoded'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a57c3f0-98c1-49eb-a2f6-77749db0a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (75.8.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626ea59c-8733-4118-8f25-5b1546941c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Cr√©er le tokenizer (limite vocabulaire √† 10 000 mots)\n",
    "# tokenizer = Tokenizer(num_words=10000)  #si par exemple [1,10,50,33] il devient [1,10,50,33,0,0,0,....,0]\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# # Convertir texte en s√©quences d'entiers\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# # Padding pour que toutes les s√©quences aient la m√™me longueur (ex: max 100 mots)\n",
    "# max_len = 100\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "# X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880e82ac-dade-4c24-bc36-8989f9954ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # Couche d'embedding : transforme chaque mot (indice) en vecteur dense de dimension 128\n",
    "# model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_len))\n",
    "\n",
    "# # Couche LSTM avec 64 unit√©s (neurones)\n",
    "# model.add(LSTM(64)) #64 neuron dans sla couche\n",
    "\n",
    "# # Couche Dense finale avec activation softmax pour classification multi-classes\n",
    "# model.add(Dense(len(le.classes_), activation='softmax')) # une vecteur de 1 ligne et 4 colonne avec probabilite de message soit dans chaque category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f523ab9a-615d-4285-b68d-4e427c3b1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abffbb83-7f98-40f4-a6a5-91d9c1bdd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c04886b-530b-437d-9f83-a86ae77da768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "# print(f\"Test accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b6da69-e6dd-44c0-8c47-f0a5b05b1abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6299dd-6758-4de5-aaa6-8b04bf169780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20e0fead-aa1b-493e-a51e-45a802a2ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: (1401, 30)\n",
      "attention_mask shape: (1401, 30)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_texts(texts, max_len=30):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"np\"  # ‚ö†Ô∏è important pour Keras\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train)\n",
    "test_encodings = tokenize_texts(X_test)\n",
    "print(\"input_ids shape:\", train_encodings[\"input_ids\"].shape)        # Doit √™tre (num_samples, max_len)\n",
    "print(\"attention_mask shape:\", train_encodings[\"attention_mask\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e30914c-9671-421f-b185-784c1e5da038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_classes = len(le.classes_)  # ex : 4\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c4cc687-b0fd-4afb-96f2-90bd2b5808f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.8.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fd4e0d-34b8-41e2-84a8-79a0f45250dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCamembertModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing TFCamembertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFCamembertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFCamembertModel\n",
    "import tensorflow as tf\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "# Charger CamemBERT\n",
    "camembert = TFCamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Entr√©es du mod√®le\n",
    "input_ids = tf.keras.Input(shape=(30,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(30,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# üîê Rendre la fonction s√©rialisable\n",
    "@register_keras_serializable()\n",
    "def extract_cls(inputs):\n",
    "    input_ids, attention_mask = inputs\n",
    "    outputs = camembert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    cls_token = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "    return cls_token\n",
    "\n",
    "# Appliquer la couche Lambda avec la fonction s√©rialisable\n",
    "bert_output = tf.keras.layers.Lambda(\n",
    "    extract_cls,\n",
    "    output_shape=(768,),\n",
    "    name=\"extract_cls\"\n",
    ")([input_ids, attention_mask])\n",
    "\n",
    "# Reste du mod√®le\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(bert_output)\n",
    "output = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c983a45-5ad7-429b-8bf0-0cfd7ff2b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a4dd370-3d56-4e78-aa3e-7b476f176f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.2414 - loss: 1.4108 - val_accuracy: 0.2979 - val_loss: 1.3819\n",
      "Epoch 2/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.3321 - loss: 1.3692 - val_accuracy: 0.3546 - val_loss: 1.3605\n",
      "Epoch 3/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.4028 - loss: 1.3523 - val_accuracy: 0.4326 - val_loss: 1.3421\n",
      "Epoch 4/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - accuracy: 0.5206 - loss: 1.3338 - val_accuracy: 0.5319 - val_loss: 1.3256\n",
      "Epoch 5/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - accuracy: 0.5666 - loss: 1.3163 - val_accuracy: 0.5390 - val_loss: 1.3093\n",
      "Epoch 6/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.5960 - loss: 1.2986 - val_accuracy: 0.5532 - val_loss: 1.2939\n",
      "Epoch 7/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.6259 - loss: 1.2788 - val_accuracy: 0.5957 - val_loss: 1.2787\n",
      "Epoch 8/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.6782 - loss: 1.2617 - val_accuracy: 0.5887 - val_loss: 1.2629\n",
      "Epoch 9/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.7016 - loss: 1.2517 - val_accuracy: 0.6525 - val_loss: 1.2476\n",
      "Epoch 10/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - accuracy: 0.7311 - loss: 1.2249 - val_accuracy: 0.7021 - val_loss: 1.2324\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x={\n",
    "        \"input_ids\": train_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": train_encodings[\"attention_mask\"]\n",
    "    },\n",
    "    y=y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    \n",
    "    batch_size=16,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baba31e3-1ee5-4cee-bca3-e95d6f40f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3s/step - accuracy: 0.6913 - loss: 1.2379\n",
      "Accuracy: 0.6952\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(\n",
    "    x={\n",
    "        \"input_ids\": test_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": test_encodings[\"attention_mask\"]\n",
    "    },\n",
    "    y=y_test_cat\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ff3a383-8d32-4351-97c8-d20c3059b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"email_classifier_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "852654b4-82c9-4f2e-a229-a642d5e74f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e30b6a6-8f72-4646-9600-43c333a23e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./camembert_tokenizer\\\\tokenizer_config.json',\n",
       " './camembert_tokenizer\\\\special_tokens_map.json',\n",
       " './camembert_tokenizer\\\\sentencepiece.bpe.model',\n",
       " './camembert_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./camembert_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e85c888-b124-4a9d-805c-71e15ff53931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step\n",
      "Cat√©gorie: autre, Confiance: 0.31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def predire_categorie(texte):\n",
    "    # Nettoyer le texte\n",
    "    texte_clean = nettoyer_texte(texte)\n",
    "    \n",
    "    # Tokeniser\n",
    "    encoded = tokenizer(\n",
    "        texte_clean,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=30,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    \n",
    "    # Pr√©dire\n",
    "    prediction = model.predict({\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"]\n",
    "    })\n",
    "    \n",
    "    # Retourner la cat√©gorie\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    return le.inverse_transform([predicted_class])[0], confidence\n",
    "\n",
    "# Test\n",
    "resultat, confiance = predire_categorie(\"\")\n",
    "print(f\"Cat√©gorie: {resultat}, Confiance: {confiance:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "846efcd2-5532-4fbd-88c6-9fdf62285684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af505127-839e-4713-803d-8ce34e371538",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5493f8-4488-4350-aaaf-d378ed4581f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
